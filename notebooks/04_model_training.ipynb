{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc07447e",
   "metadata": {},
   "source": [
    "# Model Selection & Training\n",
    "\n",
    "**Objective:** Train and tune models to predict Airbnb new user booking destinations.\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Load the preprocessed data\n",
    "2. Split the data into training and validation sets\n",
    "3. Train various machine learning models\n",
    "4. Tune hyperparameters using cross-validation\n",
    "5. Save the trained models for later evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd66040",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bb941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Import ML libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# For hyperparameter optimization\n",
    "import optuna\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set(style=\"whitegrid\", palette=\"muted\")\n",
    "\n",
    "# Display progress bars for longer operations\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d226ac",
   "metadata": {},
   "source": [
    "## 2. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdf8768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed training data\n",
    "processed_data_path = '../data/processed/processed_train_users.csv'\n",
    "\n",
    "if os.path.exists(processed_data_path):\n",
    "    df = pd.read_csv(processed_data_path)\n",
    "    print(f\"Loaded preprocessed data with shape: {df.shape}\")\n",
    "else:\n",
    "    print(f\"Error: Preprocessed data file not found at {processed_data_path}\")\n",
    "    print(\"Please run the 02_preprocessing.ipynb notebook first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcd4f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check of the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c6cf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of target variable\n",
    "plt.figure(figsize=(12, 6))\n",
    "target_counts = df['country_destination'].value_counts()\n",
    "sns.barplot(x=target_counts.index, y=target_counts.values)\n",
    "plt.title('Distribution of Country Destinations')\n",
    "plt.xlabel('Country Destination')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Print percentages\n",
    "print(\"Class distribution:\")\n",
    "print(df['country_destination'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0ff5e3",
   "metadata": {},
   "source": [
    "## 3. Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e15069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "X = df.drop(['country_destination', 'id'], axis=1, errors='ignore')  # Drop target and user id\n",
    "y = df['country_destination']\n",
    "\n",
    "# Check if there are any remaining non-numeric columns\n",
    "non_numeric_cols = X.select_dtypes(exclude=['number']).columns.tolist()\n",
    "print(f\"Non-numeric columns that need encoding: {non_numeric_cols}\")\n",
    "\n",
    "# If there are categorical columns that weren't one-hot encoded in preprocessing\n",
    "if non_numeric_cols:\n",
    "    X = pd.get_dummies(X, columns=non_numeric_cols, drop_first=True)\n",
    "    print(f\"After one-hot encoding, X shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865d3199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets using stratified sampling\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9d7d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any NaN values\n",
    "print(f\"NaN values in training features: {X_train.isna().sum().sum()}\")\n",
    "print(f\"NaN values in validation features: {X_val.isna().sum().sum()}\")\n",
    "\n",
    "# Replace any remaining NaN values with 0\n",
    "if X_train.isna().sum().sum() > 0:\n",
    "    X_train = X_train.fillna(0)\n",
    "    X_val = X_val.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f039b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Save the scaler for future use\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "with open('../models/scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6d1a0d",
   "metadata": {},
   "source": [
    "## 4. Baseline Models\n",
    "\n",
    "We'll start with simple baseline models before moving to more complex ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c828ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate and display model performance\n",
    "def evaluate_model(model, X_train, X_val, y_train, y_val, model_name):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score (weighted): {f1:.4f}\")\n",
    "    \n",
    "    # Display confusion matrix\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_)\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_val, y_pred))\n",
    "    \n",
    "    return model, accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bade4ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Logistic Regression (Baseline)\n",
    "lr_model = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs', random_state=42)\n",
    "lr_model, lr_accuracy, lr_f1 = evaluate_model(lr_model, X_train_scaled, X_val_scaled, y_train, y_val, \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ec6565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Random Forest (Baseline)\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model, rf_accuracy, rf_f1 = evaluate_model(rf_model, X_train, X_val, y_train, y_val, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aeab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 XGBoost (Baseline)\n",
    "xgb_model = xgb.XGBClassifier(objective='multi:softmax', random_state=42, n_jobs=-1)\n",
    "xgb_model, xgb_accuracy, xgb_f1 = evaluate_model(xgb_model, X_train, X_val, y_train, y_val, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a8826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 LightGBM (Baseline)\n",
    "lgb_model = lgb.LGBMClassifier(objective='multiclass', random_state=42, n_jobs=-1)\n",
    "lgb_model, lgb_accuracy, lgb_f1 = evaluate_model(lgb_model, X_train, X_val, y_train, y_val, \"LightGBM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2241d02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline models\n",
    "baseline_results = {\n",
    "    'Model': ['Logistic Regression', 'Random Forest', 'XGBoost', 'LightGBM'],\n",
    "    'Accuracy': [lr_accuracy, rf_accuracy, xgb_accuracy, lgb_accuracy],\n",
    "    'F1 Score': [lr_f1, rf_f1, xgb_f1, lgb_f1]\n",
    "}\n",
    "\n",
    "baseline_df = pd.DataFrame(baseline_results)\n",
    "baseline_df = baseline_df.sort_values('F1 Score', ascending=False).reset_index(drop=True)\n",
    "baseline_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ce7951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize baseline model comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "x = baseline_df['Model']\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x=np.arange(len(x)), height=baseline_df['Accuracy'], width=width, label='Accuracy', alpha=0.7)\n",
    "plt.bar(x=np.arange(len(x)) + width, height=baseline_df['F1 Score'], width=width, label='F1 Score', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Baseline Model Performance Comparison')\n",
    "plt.xticks(np.arange(len(x)) + width/2, x, rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f36621",
   "metadata": {},
   "source": [
    "## 5. Feature Importance Analysis\n",
    "\n",
    "Let's analyze feature importance for the best-performing model from our baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd42be27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot feature importance\n",
    "def plot_feature_importance(model, feature_names, model_name, top_n=20):\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        # For tree-based models\n",
    "        importances = model.feature_importances_\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        # For linear models like Logistic Regression\n",
    "        importances = np.mean(np.abs(model.coef_), axis=0)\n",
    "    else:\n",
    "        print(f\"Model {model_name} does not have feature importances available.\")\n",
    "        return\n",
    "    \n",
    "    # Get feature importances and names\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    })\n",
    "    \n",
    "    # Sort by importance and get top N\n",
    "    feature_importance = feature_importance.sort_values('Importance', ascending=False).head(top_n)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
    "    plt.title(f'Top {top_n} Feature Importance - {model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d8453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the best model from baselines\n",
    "best_baseline_model_name = baseline_df.iloc[0]['Model']\n",
    "print(f\"Best performing baseline model: {best_baseline_model_name}\")\n",
    "\n",
    "# Get the corresponding model object\n",
    "if best_baseline_model_name == 'Logistic Regression':\n",
    "    best_model = lr_model\n",
    "elif best_baseline_model_name == 'Random Forest':\n",
    "    best_model = rf_model\n",
    "elif best_baseline_model_name == 'XGBoost':\n",
    "    best_model = xgb_model\n",
    "else:  # LightGBM\n",
    "    best_model = lgb_model\n",
    "\n",
    "# Plot feature importance for the best model\n",
    "feature_importance_df = plot_feature_importance(best_model, X_train.columns, best_baseline_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5ff95f",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning with Cross-Validation\n",
    "\n",
    "Let's tune the hyperparameters of our best-performing model to improve its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645f43c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best baseline model to tune\n",
    "best_model_name = best_baseline_model_name\n",
    "print(f\"Tuning hyperparameters for: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644597db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Tuning Random Forest (if it's the best model)\n",
    "if best_model_name == 'Random Forest':\n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    \n",
    "    # Create model\n",
    "    rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "    \n",
    "    # Use cross-validation with grid search\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=cv, \n",
    "                              scoring='f1_weighted', n_jobs=-1, verbose=1)\n",
    "    \n",
    "    # Train with grid search\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get best model and parameters\n",
    "    best_rf = grid_search.best_estimator_\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Evaluate tuned model\n",
    "    tuned_model, tuned_accuracy, tuned_f1 = evaluate_model(best_rf, X_train, X_val, y_train, y_val, \"Tuned Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e2cb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Tuning XGBoost (if it's the best model)\n",
    "if best_model_name == 'XGBoost':\n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'subsample': [0.7, 0.8, 0.9],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "    }\n",
    "    \n",
    "    # Create model\n",
    "    xgb_clf = xgb.XGBClassifier(objective='multi:softmax', random_state=42, n_jobs=-1)\n",
    "    \n",
    "    # Use cross-validation with grid search\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=xgb_clf, param_grid=param_grid, cv=cv, \n",
    "                              scoring='f1_weighted', n_jobs=-1, verbose=1)\n",
    "    \n",
    "    # Train with grid search\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get best model and parameters\n",
    "    best_xgb = grid_search.best_estimator_\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Evaluate tuned model\n",
    "    tuned_model, tuned_accuracy, tuned_f1 = evaluate_model(best_xgb, X_train, X_val, y_train, y_val, \"Tuned XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbd97f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 Tuning LightGBM (if it's the best model)\n",
    "if best_model_name == 'LightGBM':\n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 5, 7, -1],  # -1 means no limit\n",
    "        'num_leaves': [31, 63, 127],\n",
    "        'subsample': [0.7, 0.8, 0.9]\n",
    "    }\n",
    "    \n",
    "    # Create model\n",
    "    lgbm = lgb.LGBMClassifier(objective='multiclass', random_state=42, n_jobs=-1)\n",
    "    \n",
    "    # Use cross-validation with grid search\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=lgbm, param_grid=param_grid, cv=cv, \n",
    "                              scoring='f1_weighted', n_jobs=-1, verbose=1)\n",
    "    \n",
    "    # Train with grid search\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get best model and parameters\n",
    "    best_lgbm = grid_search.best_estimator_\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Evaluate tuned model\n",
    "    tuned_model, tuned_accuracy, tuned_f1 = evaluate_model(best_lgbm, X_train, X_val, y_train, y_val, \"Tuned LightGBM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe7e8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4 Tuning Logistic Regression (if it's the best model)\n",
    "if best_model_name == 'Logistic Regression':\n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'solver': ['lbfgs', 'newton-cg', 'sag'],\n",
    "        'penalty': ['l2', 'none']\n",
    "    }\n",
    "    \n",
    "    # Create model\n",
    "    lr = LogisticRegression(max_iter=2000, multi_class='multinomial', random_state=42)\n",
    "    \n",
    "    # Use cross-validation with grid search\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=lr, param_grid=param_grid, cv=cv, \n",
    "                              scoring='f1_weighted', n_jobs=-1, verbose=1)\n",
    "    \n",
    "    # Train with grid search\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Get best model and parameters\n",
    "    best_lr = grid_search.best_estimator_\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Evaluate tuned model\n",
    "    tuned_model, tuned_accuracy, tuned_f1 = evaluate_model(best_lr, X_train_scaled, X_val_scaled, y_train, y_val, \"Tuned Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e807f1",
   "metadata": {},
   "source": [
    "## 7. Advanced Hyperparameter Tuning with Optuna\n",
    "\n",
    "Now let's use Optuna to perform more sophisticated hyperparameter optimization for our best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3c5712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for XGBoost\n",
    "def objective_xgb(trial):\n",
    "    param = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0.01, 1.0),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1, 10),\n",
    "        'objective': 'multi:softmax',\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    # Create model\n",
    "    model = xgb.XGBClassifier(**param)\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='f1_weighted')\n",
    "    \n",
    "    # Return mean CV score\n",
    "    return cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fef932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for LightGBM\n",
    "def objective_lgb(trial):\n",
    "    param = {\n",
    "        'objective': 'multiclass',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    # Create model\n",
    "    model = lgb.LGBMClassifier(**param)\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='f1_weighted')\n",
    "    \n",
    "    # Return mean CV score\n",
    "    return cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0840658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for RandomForest\n",
    "def objective_rf(trial):\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 25),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "        'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    # Create model\n",
    "    model = RandomForestClassifier(**param)\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='f1_weighted')\n",
    "    \n",
    "    # Return mean CV score\n",
    "    return cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11f69e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which objective function to use based on best baseline model\n",
    "if best_model_name == 'XGBoost':\n",
    "    objective_function = objective_xgb\n",
    "    print(\"Running Optuna optimization for XGBoost...\")\n",
    "elif best_model_name == 'LightGBM':\n",
    "    objective_function = objective_lgb\n",
    "    print(\"Running Optuna optimization for LightGBM...\")\n",
    "elif best_model_name == 'Random Forest':\n",
    "    objective_function = objective_rf\n",
    "    print(\"Running Optuna optimization for Random Forest...\")\n",
    "else:\n",
    "    # For Logistic Regression, we'll use GridSearchCV results\n",
    "    print(\"Skipping Optuna optimization for Logistic Regression (GridSearchCV is sufficient)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af97ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Optuna optimization if applicable\n",
    "if best_model_name in ['XGBoost', 'LightGBM', 'Random Forest']:\n",
    "    # Create study\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    \n",
    "    # Optimize with 50 trials (adjust as needed)\n",
    "    study.optimize(objective_function, n_trials=50)\n",
    "    \n",
    "    # Print best parameters\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(f\"  Value: {trial.value:.4f}\")\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "    \n",
    "    # Create model with best parameters\n",
    "    if best_model_name == 'XGBoost':\n",
    "        final_model = xgb.XGBClassifier(\n",
    "            objective='multi:softmax',\n",
    "            random_state=42,\n",
    "            **trial.params\n",
    "        )\n",
    "    elif best_model_name == 'LightGBM':\n",
    "        final_model = lgb.LGBMClassifier(\n",
    "            objective='multiclass',\n",
    "            random_state=42,\n",
    "            **trial.params\n",
    "        )\n",
    "    else:  # Random Forest\n",
    "        final_model = RandomForestClassifier(\n",
    "            random_state=42,\n",
    "            **trial.params\n",
    "        )\n",
    "    \n",
    "    # Evaluate final model\n",
    "    final_model, final_accuracy, final_f1 = evaluate_model(\n",
    "        final_model, X_train, X_val, y_train, y_val, f\"Optuna-Tuned {best_model_name}\"\n",
    "    )\n",
    "else:\n",
    "    # For Logistic Regression, use the GridSearchCV results as the final model\n",
    "    final_model = best_lr if 'best_lr' in locals() else lr_model\n",
    "    final_accuracy = tuned_accuracy if 'tuned_accuracy' in locals() else lr_accuracy\n",
    "    final_f1 = tuned_f1 if 'tuned_f1' in locals() else lr_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e889fa8",
   "metadata": {},
   "source": [
    "## 8. Retrain Final Model on Full Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365c6097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain final model on the full training dataset\n",
    "print(f\"Training final {best_model_name} model on full training data...\")\n",
    "final_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a5456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "model_filename = f\"../models/final_{best_model_name.lower().replace(' ', '_')}_model.pkl\"\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(final_model, f)\n",
    "\n",
    "print(f\"Final model saved to {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1809e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model metadata\n",
    "model_meta = {\n",
    "    'model_type': best_model_name,\n",
    "    'accuracy': final_accuracy,\n",
    "    'f1_score': final_f1,\n",
    "    'training_date': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    'model_file': model_filename,\n",
    "    'feature_count': X_train.shape[1],\n",
    "    'train_samples': X_train.shape[0],\n",
    "    'validation_samples': X_val.shape[0],\n",
    "    'classes': list(final_model.classes_)\n",
    "}\n",
    "\n",
    "meta_filename = \"../models/model_metadata.json\"\n",
    "import json\n",
    "with open(meta_filename, 'w') as f:\n",
    "    json.dump(model_meta, f, indent=4)\n",
    "\n",
    "print(f\"Model metadata saved to {meta_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02faad5",
   "metadata": {},
   "source": [
    "## 9. Final Performance on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2b8ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on validation set\n",
    "y_pred = final_model.predict(X_val)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Final Model ({best_model_name}) Performance on Validation Set:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score (weighted): {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e28b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final confusion matrix\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=final_model.classes_, yticklabels=final_model.classes_)\n",
    "plt.title(f'Final Confusion Matrix - {best_model_name}')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"../outputs/final_confusion_matrix.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6121bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display classification report\n",
    "print(\"Final Classification Report:\")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcf3eba",
   "metadata": {},
   "source": [
    "## 10. Predict Top 5 Countries (Kaggle Evaluation Metric)\n",
    "\n",
    "The Kaggle competition uses NDCG@5 (Normalized Discounted Cumulative Gain) as the evaluation metric, so we need to predict the top 5 most likely destinations for each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9ef33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get top 5 predictions for each sample\n",
    "def get_top5_predictions(model, X):\n",
    "    # Get probability scores for each class\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        proba = model.predict_proba(X)\n",
    "    else:\n",
    "        # For models without predict_proba\n",
    "        print(\"Warning: Model doesn't have predict_proba. Using alternative approach.\")\n",
    "        # Example alternative for XGBoost:\n",
    "        if isinstance(model, xgb.XGBClassifier):\n",
    "            proba = model.predict_proba(X)\n",
    "        else:\n",
    "            raise ValueError(\"Model doesn't support probability predictions\")\n",
    "    \n",
    "    # Get classes\n",
    "    classes = model.classes_\n",
    "    \n",
    "    # Get top 5 indices\n",
    "    top5_indices = np.argsort(-proba, axis=1)[:, :5]\n",
    "    \n",
    "    # Convert indices to class labels\n",
    "    top5_predictions = np.take(classes, top5_indices)\n",
    "    \n",
    "    return top5_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62bcfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 5 predictions for validation set\n",
    "top5_preds = get_top5_predictions(final_model, X_val)\n",
    "\n",
    "# Check a few examples\n",
    "for i in range(min(5, len(y_val))):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"True destination: {y_val.iloc[i]}\")\n",
    "    print(f\"Top 5 predictions: {top5_preds[i]}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e6f435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate how often the true label is in the top 5 predictions\n",
    "hits = 0\n",
    "for i, true_label in enumerate(y_val):\n",
    "    if true_label in top5_preds[i]:\n",
    "        hits += 1\n",
    "\n",
    "top5_accuracy = hits / len(y_val)\n",
    "print(f\"Top-5 Accuracy: {top5_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37f73d3",
   "metadata": {},
   "source": [
    "## 11. Summary of Results\n",
    "\n",
    "Let's summarize our model training process and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b14df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary DataFrame\n",
    "summary = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'F1 Score (weighted)', 'Top-5 Accuracy'],\n",
    "    'Value': [accuracy, f1, top5_accuracy]\n",
    "})\n",
    "\n",
    "print(f\"Final Model: {best_model_name}\")\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7efaa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot final feature importance\n",
    "if hasattr(final_model, 'feature_importances_') or hasattr(final_model, 'coef_'):\n",
    "    feature_importance_df = plot_feature_importance(final_model, X_train.columns, f\"Final {best_model_name}\", top_n=20)\n",
    "    \n",
    "    # Save feature importance plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importance_df)\n",
    "    plt.title(f'Top 20 Feature Importance - Final {best_model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../outputs/feature_importance.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15d7134",
   "metadata": {},
   "source": [
    "## 12. Save Model Pipeline for Inference\n",
    "\n",
    "Finally, let's save the complete pipeline for making predictions on new users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba31aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with all components needed for inference\n",
    "pipeline_components = {\n",
    "    'model': final_model,\n",
    "    'scaler': scaler,\n",
    "    'feature_names': X_train.columns.tolist(),\n",
    "    'target_classes': final_model.classes_.tolist()\n",
    "}\n",
    "\n",
    "# Save the pipeline\n",
    "pipeline_filename = \"../models/prediction_pipeline.pkl\"\n",
    "with open(pipeline_filename, 'wb') as f:\n",
    "    pickle.dump(pipeline_components, f)\n",
    "\n",
    "print(f\"Prediction pipeline saved to {pipeline_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0886bec",
   "metadata": {},
   "source": [
    "## 13. Next Steps\n",
    "\n",
    "Here are the next steps for the Airbnb New User Booking Prediction project:\n",
    "\n",
    "1. **Evaluation on Test Set**: Apply the trained model to the test set in `05_evaluation.ipynb`.\n",
    "2. **Prepare Kaggle Submission**: Format predictions according to the competition requirements.\n",
    "3. **Feature Engineering Extensions**: Consider additional features or engineering approaches:\n",
    "   - Try more sophisticated session data aggregation\n",
    "   - Extract more date-based features\n",
    "   - Consider NLP features from text fields if available\n",
    "4. **Ensemble Methods**: Combine multiple models for improved performance.\n",
    "5. **Deep Learning**: Consider neural network approaches for this multiclass classification problem.\n",
    "\n",
    "The code developed in this notebook provides a solid foundation for multiclass classification with comprehensive validation and evaluation metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
