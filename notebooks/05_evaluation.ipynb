{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96fb0705",
   "metadata": {},
   "source": [
    "# Airbnb New User Booking Destinations - Model Evaluation\n",
    "\n",
    "This notebook focuses on evaluating the models trained in the previous notebook and selecting the best performing model for predicting Airbnb new user booking destinations.\n",
    "\n",
    "## Objectives:\n",
    "- Load the trained models\n",
    "- Evaluate their performance using various metrics\n",
    "- Compare models and select the best one\n",
    "- Generate predictions for the test set\n",
    "- Prepare submission file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0c9125",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dd9f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba990ab",
   "metadata": {},
   "source": [
    "## Load Preprocessed Data\n",
    "\n",
    "First, let's load the preprocessed training and test data that we prepared in the previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf04bc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed training and test data\n",
    "X_train = pd.read_csv('../data/processed/X_train_preprocessed.csv')\n",
    "X_test = pd.read_csv('../data/processed/X_test_preprocessed.csv')\n",
    "y_train = pd.read_csv('../data/processed/y_train.csv', squeeze=True)\n",
    "X_val = pd.read_csv('../data/processed/X_val_preprocessed.csv')\n",
    "y_val = pd.read_csv('../data/processed/y_val.csv', squeeze=True)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Validation data shape: {X_val.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d79ba36",
   "metadata": {},
   "source": [
    "## Load Trained Models\n",
    "\n",
    "Let's load the models we trained in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907762ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to store models if it doesn't exist\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Function to load models\n",
    "def load_model(model_path):\n",
    "    try:\n",
    "        return joblib.load(model_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from {model_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load all models\n",
    "models = {\n",
    "    'Logistic Regression': load_model('../models/logistic_regression_model.pkl'),\n",
    "    'Random Forest': load_model('../models/random_forest_model.pkl'),\n",
    "    'XGBoost': load_model('../models/xgboost_model.pkl'),\n",
    "    'LightGBM': load_model('../models/lightgbm_model.pkl')\n",
    "}\n",
    "\n",
    "# Print loaded models\n",
    "for model_name, model in models.items():\n",
    "    if model is not None:\n",
    "        print(f\"Loaded model: {model_name}\")\n",
    "    else:\n",
    "        print(f\"Failed to load model: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c91317d",
   "metadata": {},
   "source": [
    "## Evaluation Functions\n",
    "\n",
    "Define functions to evaluate and compare the performance of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70651166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y, model_name):\n",
    "    \"\"\"Evaluate a model's performance on the given dataset\"\"\"\n",
    "    if model is None:\n",
    "        print(f\"Model {model_name} is not available for evaluation\")\n",
    "        return {}\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y, y_pred),\n",
    "        'f1_macro': f1_score(y, y_pred, average='macro'),\n",
    "        'f1_weighted': f1_score(y, y_pred, average='weighted'),\n",
    "        'confusion_matrix': confusion_matrix(y, y_pred),\n",
    "        'classification_report': classification_report(y, y_pred, output_dict=True)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def ndcg_at_k(y_true, y_pred_proba, k=5):\n",
    "    \"\"\"Calculate NDCG@k metric for a multiclass prediction\"\"\"\n",
    "    # This is a simplified version for illustration\n",
    "    # In real applications, you might want to use a library like scikit-learn or implement the full algorithm\n",
    "    if len(y_pred_proba.shape) < 2:\n",
    "        return 0\n",
    "    \n",
    "    # Get top k predictions for each sample\n",
    "    top_k_indices = np.argsort(-y_pred_proba, axis=1)[:, :k]\n",
    "    \n",
    "    # Create a mapping for label encoder if needed\n",
    "    if hasattr(models['Logistic Regression'], 'classes_'):\n",
    "        class_mapping = {i: cls for i, cls in enumerate(models['Logistic Regression'].classes_)}\n",
    "    else:\n",
    "        # If no mapping is available, we'll use indices directly\n",
    "        class_mapping = {i: i for i in range(y_pred_proba.shape[1])}\n",
    "    \n",
    "    # Calculate DCG and IDCG\n",
    "    dcg = 0\n",
    "    idcg = 1.0  # For binary relevance, IDCG@1 = 1\n",
    "    \n",
    "    for i, (true_label, pred_indices) in enumerate(zip(y_true, top_k_indices)):\n",
    "        # Check if the true label is in the top k predictions\n",
    "        if any(class_mapping[idx] == true_label for idx in pred_indices):\n",
    "            rank = 1 + next(r for r, idx in enumerate(pred_indices) if class_mapping[idx] == true_label)\n",
    "            dcg += 1.0 / np.log2(rank + 1)\n",
    "    \n",
    "    return dcg / (len(y_true) * idcg)\n",
    "\n",
    "def top_k_accuracy(y_true, y_pred_proba, k=5):\n",
    "    \"\"\"Calculate Top-K accuracy for multiclass prediction\"\"\"\n",
    "    if len(y_pred_proba.shape) < 2:\n",
    "        return 0\n",
    "    \n",
    "    top_k_indices = np.argsort(-y_pred_proba, axis=1)[:, :k]\n",
    "    \n",
    "    if hasattr(models['Logistic Regression'], 'classes_'):\n",
    "        class_mapping = {i: cls for i, cls in enumerate(models['Logistic Regression'].classes_)}\n",
    "    else:\n",
    "        class_mapping = {i: i for i in range(y_pred_proba.shape[1])}\n",
    "    \n",
    "    correct = 0\n",
    "    for i, (true_label, pred_indices) in enumerate(zip(y_true, top_k_indices)):\n",
    "        if any(class_mapping[idx] == true_label for idx in pred_indices):\n",
    "            correct += 1\n",
    "    \n",
    "    return correct / len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aaf22f",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Evaluate each model's performance on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381fde34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store all evaluation results\n",
    "evaluation_results = {}\n",
    "\n",
    "# For each model, evaluate on validation set\n",
    "for model_name, model in models.items():\n",
    "    if model is not None:\n",
    "        print(f\"Evaluating {model_name}...\")\n",
    "        evaluation_results[model_name] = evaluate_model(model, X_val, y_val, model_name)\n",
    "        \n",
    "        # Print basic metrics\n",
    "        print(f\"  Accuracy: {evaluation_results[model_name]['accuracy']:.4f}\")\n",
    "        print(f\"  F1 (macro): {evaluation_results[model_name]['f1_macro']:.4f}\")\n",
    "        print(f\"  F1 (weighted): {evaluation_results[model_name]['f1_weighted']:.4f}\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        # Calculate probability predictions for top-k metrics if the model supports predict_proba\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_pred_proba = model.predict_proba(X_val)\n",
    "            top5_acc = top_k_accuracy(y_val, y_pred_proba, k=5)\n",
    "            ndcg5 = ndcg_at_k(y_val, y_pred_proba, k=5)\n",
    "            \n",
    "            evaluation_results[model_name]['top5_accuracy'] = top5_acc\n",
    "            evaluation_results[model_name]['ndcg@5'] = ndcg5\n",
    "            \n",
    "            print(f\"  Top-5 Accuracy: {top5_acc:.4f}\")\n",
    "            print(f\"  NDCG@5: {ndcg5:.4f}\")\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed35815b",
   "metadata": {},
   "source": [
    "## Confusion Matrix Visualization\n",
    "\n",
    "Visualize the confusion matrix for each model to better understand where they make mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e70cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, model_name, normalize=False, figsize=(10, 8)):\n",
    "    \"\"\"Plot confusion matrix for a given model\"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        title = f'Normalized Confusion Matrix - {model_name}'\n",
    "    else:\n",
    "        title = f'Confusion Matrix - {model_name}'\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='.2f' if normalize else 'd', \n",
    "                cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    \n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.ylabel('True Label', fontsize=14)\n",
    "    plt.xlabel('Predicted Label', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrices for each model\n",
    "for model_name, results in evaluation_results.items():\n",
    "    if 'confusion_matrix' in results:\n",
    "        # Get unique classes from validation set\n",
    "        classes = np.unique(y_val)\n",
    "        \n",
    "        # Plot non-normalized confusion matrix\n",
    "        plot_confusion_matrix(results['confusion_matrix'], classes, model_name, normalize=False)\n",
    "        \n",
    "        # Plot normalized confusion matrix\n",
    "        plot_confusion_matrix(results['confusion_matrix'], classes, model_name, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c65fa4e",
   "metadata": {},
   "source": [
    "## Comparative Analysis\n",
    "\n",
    "Compare all models side by side to identify the best performer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f3d8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for comparison\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, results in evaluation_results.items():\n",
    "    row = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': results.get('accuracy', 0),\n",
    "        'F1 (macro)': results.get('f1_macro', 0),\n",
    "        'F1 (weighted)': results.get('f1_weighted', 0),\n",
    "        'Top-5 Accuracy': results.get('top5_accuracy', 0),\n",
    "        'NDCG@5': results.get('ndcg@5', 0)\n",
    "    }\n",
    "    comparison_data.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df.set_index('Model', inplace=True)\n",
    "\n",
    "# Display the comparison table\n",
    "print(\"Model Performance Comparison:\")\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bef3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot for model comparison\n",
    "metrics = ['Accuracy', 'F1 (macro)', 'F1 (weighted)', 'Top-5 Accuracy', 'NDCG@5']\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    if metric in comparison_df.columns:\n",
    "        plt.subplot(len(metrics), 1, i+1)\n",
    "        sns.barplot(x=comparison_df.index, y=comparison_df[metric])\n",
    "        plt.title(f'Model Comparison - {metric}')\n",
    "        plt.ylabel(metric)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d898f6",
   "metadata": {},
   "source": [
    "## Select Best Model\n",
    "\n",
    "Based on the evaluation metrics, select the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f935fc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best model based on F1 (macro) score\n",
    "best_model_name = comparison_df['F1 (macro)'].idxmax()\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"Best Model based on F1 (macro): {best_model_name}\")\n",
    "print(f\"F1 (macro) score: {comparison_df.loc[best_model_name, 'F1 (macro)']:.4f}\")\n",
    "\n",
    "# Check if Top-5 Accuracy might be a better metric for this task\n",
    "if 'Top-5 Accuracy' in comparison_df.columns:\n",
    "    best_model_top5 = comparison_df['Top-5 Accuracy'].idxmax()\n",
    "    print(f\"\\nBest Model based on Top-5 Accuracy: {best_model_top5}\")\n",
    "    print(f\"Top-5 Accuracy: {comparison_df.loc[best_model_top5, 'Top-5 Accuracy']:.4f}\")\n",
    "\n",
    "# For Airbnb destination prediction, Top-5 Accuracy might be more relevant\n",
    "# Let's decide on our final model\n",
    "final_model_name = best_model_top5 if 'Top-5 Accuracy' in comparison_df.columns else best_model_name\n",
    "final_model = models[final_model_name]\n",
    "\n",
    "print(f\"\\nSelected Final Model: {final_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b3fbcf",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis\n",
    "\n",
    "If the selected model supports feature importance, let's analyze which features contribute most to the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08870d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, feature_names, model_name, top_n=20):\n",
    "    \"\"\"Plot feature importance for tree-based models\"\"\"\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        # Get feature importance\n",
    "        importances = model.feature_importances_\n",
    "        \n",
    "        # Create a DataFrame for easier manipulation\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importances\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Plot top N features\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(top_n))\n",
    "        plt.title(f'Top {top_n} Feature Importance - {model_name}', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return feature_importance_df\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        # For linear models like Logistic Regression\n",
    "        coefs = model.coef_\n",
    "        \n",
    "        # For multiclass, take the average of absolute coefficients across classes\n",
    "        if len(coefs.shape) > 1:\n",
    "            importances = np.mean(np.abs(coefs), axis=0)\n",
    "        else:\n",
    "            importances = np.abs(coefs)\n",
    "        \n",
    "        # Create a DataFrame for easier manipulation\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importances\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Plot top N features\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(top_n))\n",
    "        plt.title(f'Top {top_n} Feature Importance - {model_name}', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return feature_importance_df\n",
    "    else:\n",
    "        print(f\"Model {model_name} doesn't provide feature importance information\")\n",
    "        return None\n",
    "\n",
    "# Plot feature importance for the final model\n",
    "feature_names = X_train.columns\n",
    "importance_df = plot_feature_importance(final_model, feature_names, final_model_name)\n",
    "\n",
    "if importance_df is not None:\n",
    "    print(\"Top 10 Most Important Features:\")\n",
    "    importance_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fe8fa4",
   "metadata": {},
   "source": [
    "## Save the Best Model\n",
    "\n",
    "Save the best performing model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137557c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "now = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "best_model_path = f'../models/best_model_{now}.pkl'\n",
    "\n",
    "joblib.dump(final_model, best_model_path)\n",
    "print(f\"Best model saved to {best_model_path}\")\n",
    "\n",
    "# Also save model metadata\n",
    "model_metadata = {\n",
    "    'model_name': final_model_name,\n",
    "    'accuracy': comparison_df.loc[final_model_name, 'Accuracy'],\n",
    "    'f1_macro': comparison_df.loc[final_model_name, 'F1 (macro)'],\n",
    "    'f1_weighted': comparison_df.loc[final_model_name, 'F1 (weighted)'],\n",
    "    'timestamp': now,\n",
    "    'features': list(X_train.columns)\n",
    "}\n",
    "\n",
    "if 'Top-5 Accuracy' in comparison_df.columns:\n",
    "    model_metadata['top5_accuracy'] = comparison_df.loc[final_model_name, 'Top-5 Accuracy']\n",
    "\n",
    "# Save metadata\n",
    "with open(f'../models/best_model_metadata_{now}.json', 'w') as f:\n",
    "    import json\n",
    "    json.dump(model_metadata, f, indent=4)\n",
    "\n",
    "print(\"Model metadata saved successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b9f9c9",
   "metadata": {},
   "source": [
    "## Generate Predictions for Test Set\n",
    "\n",
    "Use the best model to make predictions on the test set and prepare the submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c99dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original test data to get the user IDs\n",
    "test_users = pd.read_csv('../test_users.csv')\n",
    "test_user_ids = test_users['id']\n",
    "\n",
    "# Make predictions with the best model\n",
    "if hasattr(final_model, 'predict_proba'):\n",
    "    # Get probability predictions\n",
    "    y_test_proba = final_model.predict_proba(X_test)\n",
    "    \n",
    "    # Get the top 5 predictions for each user\n",
    "    n_classes = y_test_proba.shape[1]\n",
    "    \n",
    "    # Get the class names (country destinations)\n",
    "    if hasattr(final_model, 'classes_'):\n",
    "        class_names = final_model.classes_\n",
    "    else:\n",
    "        # If class names are not available, use indices\n",
    "        print(\"Warning: Class names not found. Using numerical indices instead.\")\n",
    "        class_names = np.arange(n_classes)\n",
    "    \n",
    "    # Create a list to store the submissions\n",
    "    submissions = []\n",
    "    \n",
    "    # For each user, get the top 5 predicted destinations\n",
    "    for i, user_id in enumerate(test_user_ids):\n",
    "        probas = y_test_proba[i]\n",
    "        top5_indices = np.argsort(-probas)[:5]  # Descending order\n",
    "        top5_destinations = [class_names[idx] for idx in top5_indices]\n",
    "        \n",
    "        # Add to submission list\n",
    "        submissions.append({\n",
    "            'id': user_id,\n",
    "            'country': ' '.join(top5_destinations)\n",
    "        })\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    submission_df = pd.DataFrame(submissions)\n",
    "else:\n",
    "    # If model doesn't support predict_proba, just use predict\n",
    "    y_test_pred = final_model.predict(X_test)\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    submission_df = pd.DataFrame({\n",
    "        'id': test_user_ids,\n",
    "        'country': y_test_pred\n",
    "    })\n",
    "\n",
    "# Display first few rows of the submission file\n",
    "print(\"Submission file preview:\")\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfe8383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the submission file\n",
    "submission_path = f'../outputs/submission_{now}.csv'\n",
    "os.makedirs('../outputs', exist_ok=True)\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"Submission file saved to {submission_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c82157",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "Summarize the findings and provide conclusions about the model evaluation process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0917050",
   "metadata": {},
   "source": [
    "### Evaluation Summary\n",
    "\n",
    "In this notebook, we've evaluated several machine learning models for predicting Airbnb new user booking destinations:\n",
    "\n",
    "1. **Models Evaluated**:\n",
    "   - Logistic Regression\n",
    "   - Random Forest\n",
    "   - XGBoost\n",
    "   - LightGBM\n",
    "\n",
    "2. **Evaluation Metrics**:\n",
    "   - Accuracy\n",
    "   - F1-Score (Macro and Weighted)\n",
    "   - Top-5 Accuracy\n",
    "   - NDCG@5\n",
    "\n",
    "3. **Best Performing Model**:\n",
    "   - We selected our final model based primarily on Top-5 Accuracy, as this is most relevant for the Airbnb prediction task where suggesting a set of potential destinations is more valuable than predicting a single destination.\n",
    "\n",
    "4. **Feature Importance**:\n",
    "   - We analyzed which features had the most predictive power in our model, helping us understand what factors influence new users' booking destinations.\n",
    "\n",
    "5. **Submission File**:\n",
    "   - We generated predictions for the test set and prepared a submission file in the required format.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "If we wanted to improve our model further, we could consider:\n",
    "\n",
    "1. **Feature Engineering**: Develop more sophisticated features based on the sessions data.\n",
    "2. **Hyperparameter Tuning**: Conduct more extensive hyperparameter optimization.\n",
    "3. **Ensemble Methods**: Create an ensemble of our best models to further improve performance.\n",
    "4. **Deep Learning**: Experiment with neural networks for this classification task.\n",
    "\n",
    "The code and model for this project have been saved and can be used for making predictions on new data or deployed as part of a user recommendation system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
